{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import of necessary python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from rich.progress import track\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the subdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdatasets = [\n",
    "    \"FD001\",\n",
    "    \"FD002\",\n",
    "    \"FD003\",\n",
    "    \"FD004\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUL_max = 125\n",
    "features = 17\n",
    "rtf = {\n",
    "    \"subdataset\": \"FD001\",\n",
    "    \"unit\": 24\n",
    "}\n",
    "cwd = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete processed data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdataset in track(subdatasets, \"Deleting old data...\"):\n",
    "    if os.path.exists(f\"{cwd}/{subdataset}\"):\n",
    "        shutil.rmtree(f\"{cwd}/{subdataset}\")\n",
    "    os.mkdir(f\"{cwd}/{subdataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over all subdatasets and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdataset in track(subdatasets, \"Processing data...\"):\n",
    "    print(f\"Processing {subdataset}...\")\n",
    "\n",
    "    # Defining window size\n",
    "    window_size = 40 if subdataset == \"FD001\" or subdataset == \"FD003\" else 60\n",
    "\n",
    "    # Initializing the min-max scaler\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    # Import the raw datasets\n",
    "    RUL = np.loadtxt(f\"{cwd}/RUL_{subdataset}.txt\")\n",
    "    train = np.loadtxt(f\"{cwd}/train_{subdataset}.txt\")\n",
    "    test = np.loadtxt(f\"{cwd}/test_{subdataset}.txt\")\n",
    "\n",
    "    # Scale the data\n",
    "    train[:, 2:] = min_max_scaler.fit_transform(train[:, 2:])\n",
    "    test[:, 2:] = min_max_scaler.transform(test[:, 2:])\n",
    "\n",
    "    # Delete sensors with irrelevant information and keep operation conditions\n",
    "    train = np.delete(train, [5, 9, 10, 14, 20, 22, 23], axis=1)\n",
    "    test = np.delete(test, [5, 9, 10, 14, 20, 22, 23], axis=1)\n",
    "\n",
    "    # Initialize new arrays\n",
    "    train_X = []\n",
    "    train_y = []\n",
    "    test_X = []\n",
    "    test_y = []\n",
    "    rtf_X = []\n",
    "    rtf_y = []\n",
    "\n",
    "    # Training set with sliding time window procedure\n",
    "    for i in range(1, int(np.max(train[:, 0])) + 1):\n",
    "        ind = np.where(train[:, 0] == i)\n",
    "        ind = ind[0]\n",
    "        data_temp = train[ind, :]\n",
    "        for j in range(len(data_temp) - window_size + 1):\n",
    "            train_X.append(data_temp[j:j + window_size, 2:].tolist())\n",
    "            train_RUL = len(data_temp) - window_size - j\n",
    "            if train_RUL > RUL_max:\n",
    "                train_RUL = RUL_max\n",
    "            train_y.append(train_RUL)\n",
    "\n",
    "    # Test set with sliding time window procedure\n",
    "    for i in range(1, int(np.max(test[:, 0])) + 1):\n",
    "        ind = np.where(test[:, 0] == i)\n",
    "        ind = ind[0]\n",
    "        data_temp = test[ind, :]\n",
    "        if len(data_temp) < window_size:\n",
    "            data_temp_a = []\n",
    "            for myi in range(data_temp.shape[1]):\n",
    "                x1 = np.linspace(0, window_size - 1, len(data_temp))\n",
    "                x_new = np.linspace(0, window_size - 1, window_size)\n",
    "                tck = interpolate.splrep(x1, data_temp[:, myi])\n",
    "                a = interpolate.splev(x_new, tck)\n",
    "                data_temp_a.append(a.tolist())\n",
    "            data_temp_a = np.array(data_temp_a)\n",
    "            data_temp = data_temp_a.T\n",
    "            data_temp = data_temp[:, 2:]\n",
    "        else:\n",
    "            data_temp = data_temp[-window_size:, 2:]\n",
    "\n",
    "        data_temp = np.reshape(data_temp, (1, data_temp.shape[0], data_temp.shape[1])) \n",
    "\n",
    "        if i == 1:\n",
    "            test_X = data_temp\n",
    "        else:\n",
    "            test_X = np.concatenate((test_X, data_temp), axis=0)\n",
    "\n",
    "        if RUL[i - 1] > RUL_max:\n",
    "            test_y.append(RUL_max)\n",
    "        else:\n",
    "            test_y.append(RUL[i - 1])\n",
    "\n",
    "    # Save the processed data\n",
    "    train_X = (np.array(train_X)).reshape(len(train_X), window_size, features)\n",
    "    train_y = (np.array(train_y)/RUL_max).transpose()\n",
    "    test_X = (np.array(test_X)).reshape(len(test_X), window_size, features)\n",
    "    test_y = (np.array(test_y)/RUL_max).transpose()\n",
    "\n",
    "    print(train_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(test_X.shape)\n",
    "    print(test_y.shape)\n",
    "\n",
    "    save_dir = f\"{cwd}/{subdataset}\"\n",
    "    with h5py.File(f\"{save_dir}/{subdataset}.h5\", 'w') as f:\n",
    "        f.create_dataset('X_train', data=train_X)\n",
    "        f.create_dataset('Y_train', data=train_y)\n",
    "        f.create_dataset('X_test', data=test_X)\n",
    "        f.create_dataset('Y_test', data=test_y)\n",
    "\n",
    "    # Creating the RTF dataset\n",
    "    if rtf[\"subdataset\"] == subdataset:\n",
    "        ind = np.where(test[:, 0] == rtf[\"unit\"])\n",
    "        ind = ind[0]\n",
    "        data_temp = test[ind, :]\n",
    "        data_RUL = RUL[rtf[\"unit\"] - 1]\n",
    "        for j in range(len(data_temp) - window_size + 1):\n",
    "            rtf_X.append(data_temp[j:j + window_size, 2:].tolist())\n",
    "            test_RUL = len(data_temp) + data_RUL - window_size - j\n",
    "            if test_RUL > RUL_max:\n",
    "                test_RUL = RUL_max\n",
    "            rtf_y.append(test_RUL)\n",
    "\n",
    "        rtf_X = (np.array(rtf_X)).reshape(len(rtf_X), window_size, features)\n",
    "        rtf_y = (np.array(rtf_y)/RUL_max).transpose()\n",
    "\n",
    "        print(rtf_X.shape)\n",
    "        print(rtf_y.shape)\n",
    "\n",
    "        with h5py.File(f\"{save_dir}/RTF.h5\", 'w') as f:\n",
    "            f.create_dataset('RTF_X', data=rtf_X)\n",
    "            f.create_dataset('RTF_Y', data=rtf_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
